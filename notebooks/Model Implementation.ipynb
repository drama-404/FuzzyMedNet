{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-05T15:41:09.046787500Z",
     "start_time": "2023-11-05T15:41:08.951433800Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import copy, math, os, pickle, time, pandas as pd, numpy as np, scipy.stats as ss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import torch, torch.utils.data as utils, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "from src.model.mmd_grud_utils import *\n",
    "from utils.eda_functions import (load_from_pickle, save_to_pickle)\n",
    "from config import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T15:41:20.240719200Z",
     "start_time": "2023-11-05T15:41:14.937151200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T15:41:23.530430700Z",
     "start_time": "2023-11-05T15:41:21.231993600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "BLINDED = False\n",
    "RANDOM = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T15:41:24.693586900Z",
     "start_time": "2023-11-05T15:41:24.521448700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "RESAMPLED_DIR = '../data/resampled/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T15:41:45.306199600Z",
     "start_time": "2023-11-05T15:41:45.147090800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "X_train = load_from_pickle(os.path.join(RESAMPLED_DIR, 'vitals_train.pkl'))\n",
    "X_dev = load_from_pickle(os.path.join(RESAMPLED_DIR, 'vitals_dev.pkl'))\n",
    "X_test = load_from_pickle(os.path.join(RESAMPLED_DIR, 'vitals_test.pkl'))\n",
    "\n",
    "Ys_train = load_from_pickle(os.path.join(RESAMPLED_DIR, 'Ys_train.pkl'))\n",
    "Ys_dev = load_from_pickle(os.path.join(RESAMPLED_DIR, 'Ys_dev.pkl'))\n",
    "Ys_test = load_from_pickle(os.path.join(RESAMPLED_DIR, 'Ys_test.pkl'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T15:41:47.933149800Z",
     "start_time": "2023-11-05T15:41:46.200827100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X_flat = X_train.reset_index()\n",
    "# X_flat.columns = X_flat.columns.droplevel(-1)\n",
    "# Y_flat = Ys_train.reset_index()\n",
    "# expanded_Y = pd.merge(X_flat, Y_flat, on=['subject_id', 'hadm_id', 'icustay_id'], how='left')\n",
    "# Y_train = expanded_Y[['subject_id', 'hadm_id', 'icustay_id', 'hours_in', 'mort_hosp']]\n",
    "# Y_train.set_index(['subject_id', 'hadm_id', 'icustay_id', 'hours_in'], inplace=True)\n",
    "# \n",
    "# X_flat = X_dev.reset_index()\n",
    "# X_flat.columns = X_flat.columns.droplevel(-1)\n",
    "# Y_flat = Ys_dev.reset_index()\n",
    "# expanded_Y = pd.merge(X_flat, Y_flat, on=['subject_id', 'hadm_id', 'icustay_id'], how='left')\n",
    "# Y_dev = expanded_Y[['subject_id', 'hadm_id', 'icustay_id', 'hours_in', 'mort_hosp']]\n",
    "# Y_dev.set_index(['subject_id', 'hadm_id', 'icustay_id', 'hours_in'], inplace=True)\n",
    "# \n",
    "# X_flat = X_test.reset_index()\n",
    "# X_flat.columns = X_flat.columns.droplevel(-1)\n",
    "# Y_flat = Ys_test.reset_index()\n",
    "# expanded_Y = pd.merge(X_flat, Y_flat, on=['subject_id', 'hadm_id', 'icustay_id'], how='left')\n",
    "# Y_test = expanded_Y[['subject_id', 'hadm_id', 'icustay_id', 'hours_in', 'mort_hosp']]\n",
    "# Y_test.set_index(['subject_id', 'hadm_id', 'icustay_id', 'hours_in'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# del Ys_train, Ys_dev, Ys_test, expanded_Y, X_flat, Y_flat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# all_cols = []\n",
    "# for i, col in enumerate(X_train.columns):\n",
    "#     if col[0] not in all_cols:\n",
    "#         all_cols.append(col[0])\n",
    "#         print(col[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x25808edc7d0>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAP_TIME          = 0  # In hours\n",
    "WINDOW_SIZE       = 48 # In hours\n",
    "SEED              = 69\n",
    "ID_COLS           = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "GPU               = '2'\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T15:41:51.953669800Z",
     "start_time": "2023-11-05T15:41:51.784186900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "\n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T15:41:54.962648200Z",
     "start_time": "2023-11-05T15:41:54.808952500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "# GRU_D_dist = DictDist({\n",
    "#     'cell_size': ss.randint(50, 75),\n",
    "#     'hidden_size': ss.randint(65, 95),\n",
    "#     'learning_rate': ss.uniform(2e-3, 1e-1),\n",
    "#     'num_epochs': ss.randint(15, 150),\n",
    "#     'patience': ss.randint(3, 7),\n",
    "#     'batch_size': ss.randint(35, 65),\n",
    "#     'early_stop_frac': ss.uniform(0.05, 0.1),\n",
    "#     'seed': ss.randint(1, 10000),\n",
    "# })\n",
    "\n",
    "EARLY_STOP_FRAC = 0.05\n",
    "SEED = 69\n",
    "\n",
    "GRU_D_dist = DictDist({\n",
    "    # 'cell_size': ss.randint(50, 75),\n",
    "    'hidden_size': ss.randint(65, 95),\n",
    "    'learning_rate': ss.loguniform(1e-4, 1e-1),\n",
    "    'num_epochs': ss.randint(15, 150),\n",
    "    'patience': ss.randint(3, 20),\n",
    "    'batch_size': ss.randint(35, 65),\n",
    "    # 'early_stop_frac': 0.1,  # Or any specific value or narrow range\n",
    "    # 'seed': 69,  # Or any fixed seed value\n",
    "    # 'grad_clip_value': ss.uniform(0.5, 1.0)\n",
    "})\n",
    "\n",
    "np.random.seed(SEED)\n",
    "GRU_D_hyperparams_list = GRU_D_dist.rvs(N)\n",
    "\n",
    "# with open('../src/model/baselines_gru-d.pkl', mode='rb') as f: results = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T15:42:57.019356500Z",
     "start_time": "2023-11-05T15:42:56.862589400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "GRU_D_hyperparams_list = [\n",
    "    {'hidden_size': 128, 'learning_rate': 0.0001, 'num_epochs': 100, 'patience': 20, 'batch_size': 32},\n",
    "    {'hidden_size': 64, 'learning_rate': 0.0001, 'num_epochs': 100, 'patience': 20, 'batch_size': 64},\n",
    "    {'hidden_size': 64, 'learning_rate': 0.0005, 'num_epochs': 100, 'patience': 10, 'batch_size': 64},\n",
    "    {'hidden_size': 128, 'learning_rate': 0.0005, 'num_epochs': 100, 'patience': 15, 'batch_size': 32},\n",
    "    {'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 100, 'patience': 15, 'batch_size': 128},\n",
    "    {'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 100, 'patience': 10, 'batch_size': 64},\n",
    "    {'hidden_size': 128, 'learning_rate': 0.001, 'num_epochs': 100, 'patience': 10, 'batch_size': 32},\n",
    "    {'hidden_size': 32, 'learning_rate': 0.001, 'num_epochs': 100, 'patience': 15, 'batch_size': 128},\n",
    "    {'hidden_size': 64, 'learning_rate': 0.001, 'num_epochs': 100, 'patience': 10, 'batch_size': 64},\n",
    "    {'hidden_size': 32, 'learning_rate': 0.01, 'num_epochs': 100, 'patience': 5, 'batch_size': 128}\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:05:57.883124700Z",
     "start_time": "2023-11-05T16:05:57.714283300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Define the parameter grid\n",
    "hidden_sizes = [32, 64, 128]\n",
    "learning_rates = [0.0001, 0.0005, 0.001, 0.01]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "# Placeholder for storing the results\n",
    "grid_search_results = []\n",
    "\n",
    "# The fixed number of epochs for training\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for learning_rate, hidden_size, batch_size in product(hidden_sizes, learning_rates, batch_sizes):\n",
    "\n",
    "    grid_search_results.append({\n",
    "        'hidden_size': hidden_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'patience': patience,\n",
    "        'num_epochs': num_epochs\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T18:16:54.413262Z",
     "start_time": "2023-11-05T18:16:54.401256500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "36"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grid_search_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T18:16:56.452288200Z",
     "start_time": "2023-11-05T18:16:56.448588300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "results = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:06:01.489510600Z",
     "start_time": "2023-11-05T16:06:01.318480Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GRU-D"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from src.model.mmd_grud_utils import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:06:03.580170300Z",
     "start_time": "2023-11-05T16:06:03.405100600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# hyperparams = {'cell_size': 72,\n",
    "#  'hidden_size': 128,\n",
    "#  'learning_rate': 0.0001,\n",
    "#  'num_epochs': 20,\n",
    "#  'patience': 10,\n",
    "#  'batch_size': 64}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model GRU-D on target mort_hosp with representation full_X\n",
      "On sample 1 / 10 (hyperparams = {'hidden_size': 128, 'learning_rate': 0.0001, 'num_epochs': 100, 'patience': 20, 'batch_size': 32})\n",
      "Model Structure:  GRUD(\n",
      "  (zl): Linear(in_features=206, out_features=128, bias=True)\n",
      "  (rl): Linear(in_features=206, out_features=128, bias=True)\n",
      "  (hl): Linear(in_features=206, out_features=128, bias=True)\n",
      "  (gamma_x_l): FilterLinear(in_features=39, out_features=39, bias=True)\n",
      "  (gamma_h_l): Linear(in_features=39, out_features=128, bias=True)\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (bn): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Start Training ... \n",
      "Output type determined by the model\n",
      "Epoch: 0, train_loss: 0.74328144, valid_loss: 0.78043734, time: [82.43], best model: 1\n",
      "Epoch: 1, train_loss: 0.63497225, valid_loss: 0.65863192, time: [73.11], best model: 1\n",
      "Epoch: 2, train_loss: 0.57226097, valid_loss: 0.6229391, time: [73.58], best model: 1\n",
      "Epoch: 3, train_loss: 0.54092805, valid_loss: 0.5871263, time: [71.2], best model: 1\n",
      "Epoch: 4, train_loss: 0.51020377, valid_loss: 0.56555536, time: [78.], best model: 1\n",
      "Epoch: 5, train_loss: 0.49453911, valid_loss: 0.53594266, time: [75.26], best model: 1\n",
      "Epoch: 6, train_loss: 0.4784805, valid_loss: 0.52144983, time: [70.97], best model: 1\n",
      "Epoch: 7, train_loss: 0.45473837, valid_loss: 0.50168329, time: [71.34], best model: 1\n",
      "Epoch: 8, train_loss: 0.44709878, valid_loss: 0.50002465, time: [71.34], best model: 1\n",
      "Epoch: 9, train_loss: 0.43376935, valid_loss: 0.48363922, time: [70.26], best model: 1\n",
      "Epoch: 10, train_loss: 0.41576842, valid_loss: 0.46884738, time: [71.01], best model: 1\n",
      "Epoch: 11, train_loss: 0.39566929, valid_loss: 0.43631501, time: [70.32], best model: 1\n",
      "Epoch: 12, train_loss: 0.37130067, valid_loss: 0.39285748, time: [70.28], best model: 1\n",
      "Epoch: 13, train_loss: 0.34887574, valid_loss: 0.37199834, time: [70.81], best model: 1\n",
      "Epoch: 14, train_loss: 0.33067538, valid_loss: 0.36377003, time: [71.61], best model: 1\n",
      "Epoch: 15, train_loss: 0.32043312, valid_loss: 0.34252699, time: [70.], best model: 1\n",
      "Epoch: 16, train_loss: 0.31911762, valid_loss: 0.33566611, time: [72.02], best model: 1\n",
      "Epoch: 17, train_loss: 0.32018641, valid_loss: 0.33940665, time: [73.84], best model: 0\n",
      "Epoch: 18, train_loss: 0.31334154, valid_loss: 0.34049382, time: [81.54], best model: 0\n",
      "Epoch: 19, train_loss: 0.30291247, valid_loss: 0.32775152, time: [72.16], best model: 1\n",
      "Epoch: 20, train_loss: 0.29726326, valid_loss: 0.32387287, time: [71.75], best model: 1\n",
      "Epoch: 21, train_loss: 0.29550477, valid_loss: 0.32421831, time: [71.41], best model: 0\n",
      "Epoch: 22, train_loss: 0.28396466, valid_loss: 0.30985503, time: [72.12], best model: 1\n",
      "Epoch: 23, train_loss: 0.29821717, valid_loss: 0.31915409, time: [72.21], best model: 0\n",
      "Epoch: 24, train_loss: 0.27681085, valid_loss: 0.30809105, time: [76.73], best model: 1\n",
      "Epoch: 25, train_loss: 0.2717303, valid_loss: 0.3004976, time: [77.32], best model: 1\n",
      "Epoch: 26, train_loss: 0.27737915, valid_loss: 0.30979427, time: [77.01], best model: 0\n",
      "Epoch: 27, train_loss: 0.38376835, valid_loss: 0.44584258, time: [75.64], best model: 0\n",
      "Epoch: 28, train_loss: 0.42619353, valid_loss: 0.46935884, time: [75.94], best model: 0\n",
      "Epoch: 29, train_loss: 0.26738406, valid_loss: 0.29122703, time: [76.1], best model: 1\n",
      "Epoch: 30, train_loss: 0.26963152, valid_loss: 0.29272082, time: [76.9], best model: 0\n",
      "Epoch: 31, train_loss: 0.27959547, valid_loss: 0.29867803, time: [75.62], best model: 0\n"
     ]
    }
   ],
   "source": [
    "model_name       = 'GRU-D'\n",
    "tasks             = ['mort_hosp']\n",
    "hyperparams_list = GRU_D_hyperparams_list\n",
    "RERUN            = False\n",
    "\n",
    "if model_name not in results: results[model_name] = {}\n",
    "\n",
    "# for t in tasks:         # Edit - DR\n",
    "#     if t not in results[model_name]: results[model_name][t] = {}\n",
    "t = 'mort_hosp'\n",
    "if t not in results[model_name]: results[model_name][t] = {}\n",
    "    # for n, X_train, X_dev, X_test in (('full_X', X_train, X_dev, X_test),):  # Edit - DR\n",
    "n, X_train, X_dev, X_test = ('full_X', X_train, X_dev, X_test)\n",
    "print(\"Running model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "X_mean = np.nanmean(\n",
    "    to_3D_tensor(\n",
    "        X_train.loc[:, pd.IndexSlice[:, 'mean']] *\n",
    "        np.where((X_train.loc[:, pd.IndexSlice[:, 'mask']] == 1).values, 1, np.NaN)\n",
    "    ),\n",
    "    axis=0, keepdims=True\n",
    ").transpose([0, 2, 1])\n",
    "base_params = {'X_mean': X_mean, 'output_last': True, 'input_size': X_mean.shape[2]}\n",
    "\n",
    "if n in results[model_name][t]:\n",
    "    if not RERUN:\n",
    "        print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "        print(results[model_name][t][n])\n",
    "        # continue  # Edit - DR\n",
    "    best_s, best_hyperparams = results[model_name][t][n][-1], results[model_name][t][n][1]\n",
    "    print(\"Loading best hyperparams\", best_hyperparams)\n",
    "else:\n",
    "    best_s, best_hyperparams = -np.Inf, None\n",
    "    for i, hyperparams in enumerate(hyperparams_list):\n",
    "        print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))\n",
    "\n",
    "        # early_stop_frac,batch_size,seed = [hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]    # Edit - DR\n",
    "        batch_size = hyperparams['batch_size']\n",
    "\n",
    "        np.random.seed(SEED)\n",
    "        all_train_subjects = list(\n",
    "            np.random.permutation(Ys_train.index.get_level_values('subject_id').values)\n",
    "        )\n",
    "        N_early_stop        = int(len(all_train_subjects) * EARLY_STOP_FRAC)\n",
    "        train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "        early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "        X_train_obs         = X_train[X_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "        Ys_train_obs        = Ys_train[Ys_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "        X_train_early_stop  = X_train[X_train.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "        Ys_train_early_stop = Ys_train[\n",
    "            Ys_train.index.get_level_values('subject_id').isin(early_stop_subjects)\n",
    "        ]\n",
    "\n",
    "        train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "        early_stop_dataloader = prepare_dataloader(\n",
    "            X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size\n",
    "        )\n",
    "        dev_dataloader        = prepare_dataloader(X_dev, Ys_dev[t], batch_size=batch_size)\n",
    "        test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "        model_hyperparams = copy.copy(base_params)\n",
    "        model_hyperparams.update(\n",
    "            {k: v for k, v in hyperparams.items() if k in ('hidden_size', 'batch_size')}\n",
    "        )\n",
    "        # del model_hyperparams['cell_size']  # Edit - DR\n",
    "        model = GRUD(**model_hyperparams)\n",
    "        \n",
    "        best_model, _ = Train_Model(\n",
    "            model, train_dataloader, early_stop_dataloader,\n",
    "            **{k: v for k, v in hyperparams.items() if k in (\n",
    "                'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "            )}\n",
    "        )\n",
    "\n",
    "        probabilities_dev, labels_dev = predict_proba(best_model, dev_dataloader)\n",
    "        probabilities_dev = np.concatenate(probabilities_dev)[:, 1]\n",
    "        labels_dev        = np.concatenate(labels_dev)\n",
    "        s = roc_auc_score(labels_dev, probabilities_dev)\n",
    "        if s > best_s:\n",
    "            best_s, best_hyperparams = s, hyperparams\n",
    "            print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-05T16:06:05.236559400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Test\n",
    "np.random.seed(SEED)\n",
    "hyperparams = best_hyperparams # In case I forgot a replace below\n",
    "# early_stop_frac,batch_size,seed = [best_hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]\n",
    "\n",
    "X_train_concat, Ys_train_concat = pd.concat((X_train, X_dev)), pd.concat((Ys_train, Ys_dev))\n",
    "\n",
    "all_train_subjects = list(np.random.permutation(Ys_train_concat.index.get_level_values('subject_id').values))\n",
    "N_early_stop = int(len(all_train_subjects) * EARLY_STOP_FRAC)\n",
    "train_subjects, early_stop_subjects = all_train_subjects[:-N_early_stop], all_train_subjects[-N_early_stop:]\n",
    "X_train_obs         = X_train_concat[X_train_concat.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "Ys_train_obs        = Ys_train_concat[Ys_train_concat.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "X_train_early_stop  = X_train_concat[X_train_concat.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "Ys_train_early_stop = Ys_train_concat[Ys_train_concat.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "\n",
    "train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "early_stop_dataloader = prepare_dataloader(X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size)\n",
    "test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "model_hyperparams = copy.copy(base_params)\n",
    "model_hyperparams.update(\n",
    "    {k: v for k, v in best_hyperparams.items() if k in ('hidden_size', 'batch_size')}\n",
    ")\n",
    "model = GRUD(**model_hyperparams)\n",
    "\n",
    "\n",
    "best_model, (losses_train, losses_early_stop, losses_epochs_train, losses_epochs_early_stop) = Train_Model(\n",
    "    model, train_dataloader, early_stop_dataloader,\n",
    "    **{k: v for k, v in best_hyperparams.items() if k in (\n",
    "        'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "    )}\n",
    ")\n",
    "\n",
    "probabilities_test, labels_test = predict_proba(best_model, test_dataloader)\n",
    "\n",
    "y_score = np.concatenate(probabilities_test)[:, 1]\n",
    "y_pred  = np.argmax(probabilities_test)\n",
    "y_true  = np.concatenate(labels_test)\n",
    "\n",
    "auc   = roc_auc_score(y_true, y_score)\n",
    "auprc = average_precision_score(y_true, y_score)\n",
    "acc   = accuracy_score(y_true, y_pred)\n",
    "F1    = f1_score(y_true, y_pred)\n",
    "print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "print(auc, auprc, acc, F1)\n",
    "\n",
    "results[model_name][t][n] = None, best_hyperparams, auc, auprc, acc, F1, best_s\n",
    "with open('../src/model/baselines_gru-d.pkl', mode='wb') as f: pickle.dump(results, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
