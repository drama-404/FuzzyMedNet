{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy, math, os, pickle, time, pandas as pd, numpy as np, scipy.stats as ss\n",
    "\n",
    "import torch, torch.utils.data as utils, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "from utils.eda_functions import (load_from_pickle, save_to_pickle)\n",
    "from config import (DATA_DIR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c68fffef59f1e886"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_3D_tensor(df):\n",
    "    idx = pd.IndexSlice\n",
    "    result = np.dstack((df.loc[idx[:,:,:,i], :].values for i in sorted(set(df.index.get_level_values('hours_in')))))\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8c137ddef38ea5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_dataloader(df, Ys, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    dfs = (df_train, df_dev, df_test).\n",
    "    df_* = (subject, hadm, icustay, hours_in) X (level2, agg fn \\ni {mask, mean, time})\n",
    "    Ys_series = (subject, hadm, icustay) => label.\n",
    "    \"\"\"\n",
    "    X     = torch.from_numpy(to_3D_tensor(df).astype(np.float32))\n",
    "    label = torch.from_numpy(Ys.values.astype(np.int64))\n",
    "    dataset = utils.TensorDataset(X, label)\n",
    "    return utils.DataLoader(dataset, batch_size=int(batch_size), shuffle=shuffle, drop_last = True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f2ad04acc6b91c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class FilterLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, filter_square_matrix, bias=True):\n",
    "        '''\n",
    "        filter_square_matrix : filter square matrix, whose each elements is 0 or 1.\n",
    "        '''\n",
    "        super(FilterLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        assert in_features > 1 and out_features > 1, \"Passing in nonsense sizes\"\n",
    "        \n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        self.filter_square_matrix = None\n",
    "        if use_gpu: self.filter_square_matrix = Variable(filter_square_matrix.cuda(), requires_grad=False)\n",
    "        else:       self.filter_square_matrix = Variable(filter_square_matrix, requires_grad=False)\n",
    "        \n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "\n",
    "        if bias: self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:    self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None: self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.filter_square_matrix.mul(self.weight),\n",
    "            self.bias\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) \\\n",
    "            + ', bias=' + str(self.bias is not None) + ')'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "878ebc47769f4924"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRUD(nn.Module):\n",
    "    def __init__(self, input_size, cell_size, hidden_size, X_mean, batch_size = 0, output_last = False):\n",
    "        \"\"\"\n",
    "        \n",
    "        GRU-D:\n",
    "            input_size: variable dimension of each time\n",
    "            hidden_size: dimension of hidden_state\n",
    "            mask_size: dimension of masking vector\n",
    "            X_mean: the mean of the historical input data_preprocessing\n",
    "        \"\"\"\n",
    "        \n",
    "        super(GRUD, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.delta_size = input_size\n",
    "        self.mask_size = input_size\n",
    "        \n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            self.identity = torch.eye(input_size).cuda()\n",
    "            self.zeros = Variable(torch.zeros(batch_size, input_size).cuda())\n",
    "            self.zeros_h = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            self.X_mean = Variable(torch.Tensor(X_mean).cuda())\n",
    "        else:\n",
    "            self.identity = torch.eye(input_size)\n",
    "            self.zeros = Variable(torch.zeros(batch_size, input_size))\n",
    "            self.zeros_h = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            self.X_mean = Variable(torch.Tensor(X_mean))\n",
    "        \n",
    "        self.zl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size) # Wz, Uz are part of the same network. the bias is bz\n",
    "        self.rl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size) # Wr, Ur are part of the same network. the bias is br\n",
    "        self.hl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size) # W, U are part of the same network. the bias is b\n",
    "        \n",
    "        self.gamma_x_l = FilterLinear(self.delta_size, self.delta_size, self.identity)\n",
    "        \n",
    "        self.gamma_h_l = nn.Linear(self.delta_size, self.hidden_size) # this was wrong in available version. remember to raise the issue\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, 2)\n",
    "        self.bn= torch.nn.BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True)\n",
    "        self.drop=nn.Dropout(p=0.5, inplace=False)\n",
    "        \n",
    "    def step(self, x, x_last_obsv, x_mean, h, mask, delta):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: input tensor\n",
    "            x_last_obsv: input tensor with forward fill applied\n",
    "            x_mean: the mean of each feature\n",
    "            h: the hidden state of the network\n",
    "            mask: the mask of whether or not the current value is observed\n",
    "            delta: the tensor indicating the number of steps since the last time a feature was observed.\n",
    "            \n",
    "        Returns:\n",
    "            h: the updated hidden state of the network\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.size()[0]\n",
    "        dim_size = x.size()[1]\n",
    "        \n",
    "        gamma_x_l_delta = self.gamma_x_l(delta)\n",
    "        delta_x = torch.exp(-torch.max(self.zeros, gamma_x_l_delta)) #exponentiated negative rectifier\n",
    "        \n",
    "        gamma_h_l_delta = self.gamma_h_l(delta)\n",
    "        delta_h = torch.exp(-torch.max(self.zeros_h, gamma_h_l_delta)) #self.zeros became self.zeros_h to accomodate hidden size != input size\n",
    "        \n",
    "        x_mean = x_mean.repeat(batch_size, 1)\n",
    "        \n",
    "        x = mask * x + (1 - mask) * (delta_x * x_last_obsv + (1 - delta_x) * x_mean)\n",
    "        h = delta_h * h\n",
    "        \n",
    "        combined = torch.cat((x, h, mask), 1)\n",
    "        z = torch.sigmoid(self.zl(combined)) #sigmoid(W_z*x_t + U_z*h_{t-1} + V_z*m_t + bz)\n",
    "        r = torch.sigmoid(self.rl(combined)) #sigmoid(W_r*x_t + U_r*h_{t-1} + V_r*m_t + br)\n",
    "        combined_new = torch.cat((x, r*h, mask), 1)\n",
    "        h_tilde = torch.tanh(self.hl(combined_new)) #tanh(W*x_t +U(r_t*h_{t-1}) + V*m_t) + b\n",
    "        h = (1 - z) * h + z * h_tilde\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def forward(self, X, X_last_obsv, Mask, Delta):\n",
    "        batch_size = X.size(0)\n",
    "#         type_size = input.size(1)\n",
    "        step_size = X.size(1) # num timepoints\n",
    "        spatial_size = X.size(2) # num feature_engineering\n",
    "        \n",
    "        Hidden_State = self.initHidden(batch_size)\n",
    "#         X = torch.squeeze(input[:,0,:,:])\n",
    "#         X_last_obsv = torch.squeeze(input[:,1,:,:])\n",
    "#         Mask = torch.squeeze(input[:,2,:,:])\n",
    "#         Delta = torch.squeeze(input[:,3,:,:])\n",
    "        \n",
    "        outputs = None\n",
    "        for i in range(step_size):\n",
    "            Hidden_State = self.step(\n",
    "                torch.squeeze(X[:,i:i+1,:], 1),\n",
    "                torch.squeeze(X_last_obsv[:,i:i+1,:], 1),\n",
    "                torch.squeeze(self.X_mean[:,i:i+1,:], 1),\n",
    "                Hidden_State,\n",
    "                torch.squeeze(Mask[:,i:i+1,:], 1),\n",
    "                torch.squeeze(Delta[:,i:i+1,:], 1),\n",
    "            )\n",
    "            if outputs is None:\n",
    "                outputs = Hidden_State.unsqueeze(1)\n",
    "            else:\n",
    "                outputs = torch.cat((Hidden_State.unsqueeze(1), outputs), 1)\n",
    "                \n",
    "        # we want to predict a binary outcome\n",
    "        #Apply 50% dropout and batch norm here\n",
    "        # self.drop(self.bn(self.fc(Hidden_State)))             # Edit - DR\n",
    "        # return self.drop(self.bn(self.fc(Hidden_State)))\n",
    "        return self.fc(Hidden_State)\n",
    "\n",
    "    #         if self.output_last:\n",
    "#             return outputs[:,-1,:]\n",
    "#         else:\n",
    "#             return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            return Hidden_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            return Hidden_State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Train_Model(\n",
    "    model, train_dataloader, valid_dataloader, num_epochs = 300, patience = 3, min_delta = 1e-5, learning_rate=1e-3, batch_size=None\n",
    "):\n",
    "    \n",
    "    print('Model Structure: ', model)\n",
    "    print('Start Training ... ')\n",
    "    \n",
    "    # model\n",
    "    \n",
    "    if (type(model) == nn.modules.container.Sequential):\n",
    "        output_last = model[-1].output_last\n",
    "        print('Output type determined by the last layer')\n",
    "    else:\n",
    "        output_last = model.output_last\n",
    "        print('Output type determined by the model')\n",
    "        \n",
    "    loss_MSE = torch.nn.MSELoss()\n",
    "    loss_nll=torch.nn.NLLLoss()\n",
    "    loss_CEL=torch.nn.CrossEntropyLoss()\n",
    "    loss_L1 = torch.nn.L1Loss()\n",
    "    \n",
    "#     optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate, alpha=0.99)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    use_gpu = False#torch.cuda.is_available()\n",
    "    \n",
    "    interval = 100\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    losses_epochs_train = []\n",
    "    losses_epochs_valid = []\n",
    "    \n",
    "    cur_time = time.time()\n",
    "    pre_time = time.time()\n",
    "    \n",
    "    # Variables for Early Stopping\n",
    "    is_best_model = 0\n",
    "    patient_epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        trained_number = 0\n",
    "        \n",
    "        valid_dataloader_iter = iter(valid_dataloader)\n",
    "        \n",
    "        losses_epoch_train = []\n",
    "        losses_epoch_valid = []\n",
    "        \n",
    "        for X, labels in train_dataloader:\n",
    "            X = X.numpy()\n",
    "            mask        = torch.from_numpy(X[:, np.arange(0, X.shape[1], 3), :].astype(np.float32))\n",
    "            measurement = torch.from_numpy(X[:, np.arange(1, X.shape[1], 3), :].astype(np.float32))\n",
    "            time_       = torch.from_numpy(X[:, np.arange(2, X.shape[1], 3), :].astype(np.float32))\n",
    "            \n",
    "            mask = torch.transpose(mask, 1, 2)\n",
    "            measurement = torch.transpose(measurement, 1, 2)\n",
    "            time_ = torch.transpose(time_, 1, 2)\n",
    "            measurement_last_obsv = measurement            \n",
    "\n",
    "            assert measurement.size()[0] == batch_size, \"Batch Size doesn't match! %s\" % str(measurement.size())\n",
    "\n",
    "            if use_gpu:\n",
    "                convert_to_cuda=lambda x: Variable(x.cuda())\n",
    "                X, X_last_obsv, Mask, Delta, labels = map(convert_to_cuda, [measurement, measurement_last_obsv, mask, time_, labels])\n",
    "            else: \n",
    "#                 inputs, labels = Variable(inputs), Variable(labels)\n",
    "                convert_to_tensor=lambda x: Variable(x)\n",
    "                X, X_last_obsv, Mask, Delta, labels  = map(convert_to_tensor, [measurement, measurement_last_obsv, mask, time_, labels])\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "            prediction=model(X, X_last_obsv, Mask, Delta)\n",
    "    \n",
    "#             print(torch.sum(torch.sum(torch.isnan(prediction))))\n",
    "            \n",
    "#             print(labels.shape)\n",
    "#             print(prediction.shape)\n",
    "            \n",
    "            if output_last:\n",
    "                print(labels)\n",
    "                print(prediction)\n",
    "                loss_train = loss_CEL(torch.squeeze(prediction), torch.squeeze(labels))\n",
    "            else:\n",
    "                print(outputs)\n",
    "                full_labels = torch.cat((inputs[:,1:,:], labels), dim = 1)\n",
    "                loss_train = loss_MSE(outputs, full_labels)\n",
    "        \n",
    "            losses_train.append(loss_train.data)\n",
    "            losses_epoch_train.append(loss_train.data)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss_train.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "             # validation \n",
    "            try: \n",
    "                X_val, labels_val = next(valid_dataloader_iter)\n",
    "                X_val = X_val.numpy()\n",
    "                mask_val        = torch.from_numpy(X_val[:, np.arange(0, X_val.shape[1], 3), :].astype(np.float32))\n",
    "                measurement_val = torch.from_numpy(X_val[:, np.arange(1, X_val.shape[1], 3), :].astype(np.float32))\n",
    "                time_val       = torch.from_numpy(X_val[:, np.arange(2, X_val.shape[1], 3), :].astype(np.float32))\n",
    "            \n",
    "                mask_val = torch.transpose(mask_val, 1, 2)\n",
    "                measurement_val = torch.transpose(measurement_val, 1, 2)\n",
    "                time_val = torch.transpose(time_val, 1, 2)\n",
    "                measurement_last_obsv_val = measurement_val\n",
    "            except StopIteration:\n",
    "                valid_dataloader_iter = iter(valid_dataloader)\n",
    "                X_val, labels_val = next(valid_dataloader_iter)\n",
    "                X_val = X_val.numpy()\n",
    "                mask_val        = torch.from_numpy(X_val[:, np.arange(0, X_val.shape[1], 3), :].astype(np.float32))\n",
    "                measurement_val = torch.from_numpy(X_val[:, np.arange(1, X_val.shape[1], 3), :].astype(np.float32))\n",
    "                time_val       = torch.from_numpy(X_val[:, np.arange(2, X_val.shape[1], 3), :].astype(np.float32))\n",
    "            \n",
    "                mask_val = torch.transpose(mask_val, 1, 2)\n",
    "                measurement_val = torch.transpose(measurement_val, 1, 2)\n",
    "                time_val = torch.transpose(time_val, 1, 2)\n",
    "                measurement_last_obsv_val = measurement_val\n",
    "            \n",
    "            if use_gpu:\n",
    "                convert_to_cuda=lambda x: Variable(x.cuda())\n",
    "                X_val, X_last_obsv_val, Mask_val, Delta_val, labels_val = map(convert_to_cuda, [measurement_val, measurement_last_obsv_val, mask_val, time_val, labels_val])\n",
    "            else: \n",
    "#                 inputs, labels = Variable(inputs), Variable(labels)\n",
    "                convert_to_tensor=lambda x: Variable(x)\n",
    "                X_val, X_last_obsv_val, Mask_val, Delta_val, labels_val = map(convert_to_tensor, [measurement_val, measurement_last_obsv_val, mask_val, time_val, labels_val])\n",
    "            \n",
    "                \n",
    "            model.zero_grad()\n",
    "            \n",
    "#             outputs_val = model(inputs_val)\n",
    "            prediction_val = model(X_val, X_last_obsv_val, Mask_val, Delta_val)\n",
    "    \n",
    "#             print(labels.shape)\n",
    "#             print(prediction_val.shape)\n",
    "            \n",
    "            if output_last:\n",
    "                loss_valid =loss_CEL(torch.squeeze(prediction_val), torch.squeeze(labels_val))\n",
    "            else:\n",
    "                raise NotImplementedError(\"Should be output last!\")\n",
    "                full_labels_val = torch.cat((inputs_val[:,1:,:], labels_val), dim = 1)\n",
    "                loss_valid = loss_MSE(outputs_val, full_labels_val)\n",
    "\n",
    "            losses_valid.append(loss_valid.data)\n",
    "            losses_epoch_valid.append(loss_valid.data)\n",
    "            \n",
    "#             print(sklearn.metrics.roc_auc_score(labels_val.detach().cpu().numpy(), prediction_val.detach().cpu().numpy()[:,1]))\n",
    "            \n",
    "            # output\n",
    "            trained_number += 1\n",
    "            \n",
    "        avg_losses_epoch_train = sum(losses_epoch_train).cpu().numpy() / float(len(losses_epoch_train))\n",
    "        avg_losses_epoch_valid = sum(losses_epoch_valid).cpu().numpy() / float(len(losses_epoch_valid))\n",
    "        losses_epochs_train.append(avg_losses_epoch_train)\n",
    "        losses_epochs_valid.append(avg_losses_epoch_valid)\n",
    "\n",
    "        # Early Stopping\n",
    "        if epoch == 0:\n",
    "            is_best_model = 1\n",
    "            best_model = model\n",
    "            min_loss_epoch_valid = 10000.0\n",
    "            if avg_losses_epoch_valid < min_loss_epoch_valid:\n",
    "                min_loss_epoch_valid = avg_losses_epoch_valid\n",
    "        else:\n",
    "            if min_loss_epoch_valid - avg_losses_epoch_valid > min_delta:\n",
    "                is_best_model = 1\n",
    "                best_model = model\n",
    "                min_loss_epoch_valid = avg_losses_epoch_valid\n",
    "                patient_epoch = 0\n",
    "            else:\n",
    "                is_best_model = 0\n",
    "                patient_epoch += 1\n",
    "                if patient_epoch >= patience:\n",
    "                    break  # Edit - DR\n",
    "        \n",
    "        # Print training parameters\n",
    "        cur_time = time.time()\n",
    "        print('Epoch: {}, train_loss: {}, valid_loss: {}, time: {}, best model: {}'.format( \\\n",
    "                    epoch, \\\n",
    "                    np.around(avg_losses_epoch_train, decimals=8),\\\n",
    "                    np.around(avg_losses_epoch_valid, decimals=8),\\\n",
    "                    np.around([cur_time - pre_time] , decimals=2),\\\n",
    "                    is_best_model) )\n",
    "        pre_time = cur_time\n",
    "#         if epoch==1:\n",
    "#             break\n",
    "                \n",
    "    return best_model, [losses_train, losses_valid, losses_epochs_train, losses_epochs_valid]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80c0f979c8a63134"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_proba(model, dataloader):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        model: GRU-D model\n",
    "        test_dataloader: containing batches of measurement, measurement_last_obsv, mask, time_, labels\n",
    "    Returns:\n",
    "        predictions: size[num_samples, 2]\n",
    "        labels: size[num_samples]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    use_gpu = False  # torch.cuda.is_available()\n",
    "    \n",
    "    probabilities = []\n",
    "    labels        = []\n",
    "    ethnicities   = []\n",
    "    genders       = []\n",
    "    for X, label in dataloader:\n",
    "        X = X.numpy()\n",
    "        mask        = torch.from_numpy(X[:, np.arange(0, X.shape[1], 3), :].astype(np.float32))\n",
    "        measurement = torch.from_numpy(X[:, np.arange(1, X.shape[1], 3), :].astype(np.float32))\n",
    "        time_       = torch.from_numpy(X[:, np.arange(2, X.shape[1], 3), :].astype(np.float32))\n",
    "\n",
    "        mask = torch.transpose(mask, 1, 2)\n",
    "        measurement = torch.transpose(measurement, 1, 2)\n",
    "        time_ = torch.transpose(time_, 1, 2)\n",
    "        measurement_last_obsv = measurement            \n",
    "\n",
    "        if use_gpu:\n",
    "            convert_to_cuda=lambda x: Variable(x.cuda())\n",
    "            X, X_last_obsv, Mask, Delta, label = map(convert_to_cuda, [measurement, measurement_last_obsv, mask, time_, label])\n",
    "        else: \n",
    "#                 inputs, labels = Variable(inputs), Variable(labels)\n",
    "            convert_to_tensor=lambda x: Variable(x)\n",
    "            X, X_last_obsv, Mask, Delta, label  = map(convert_to_tensor, [measurement, measurement_last_obsv, mask, time_, label])\n",
    "\n",
    "        \n",
    "        prob = model(X, X_last_obsv, Mask, Delta)\n",
    "        \n",
    "        probabilities.append(prob.detach().cpu().data.numpy())\n",
    "        labels.append(label.detach().cpu().data.numpy())\n",
    "\n",
    "    return probabilities, labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b49e9025ce5b72c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "\n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7adc7d2d08b53f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = load_from_pickle(os.path.join(DATA_DIR, 'Vitals_train.pkl'))\n",
    "X_dev = load_from_pickle(os.path.join(DATA_DIR, 'Vitals_dev.pkl'))\n",
    "X_test = load_from_pickle(os.path.join(DATA_DIR, 'Vitals_test.pkl'))\n",
    "\n",
    "Ys_train = load_from_pickle(os.path.join(DATA_DIR, 'Y_train.pkl'))\n",
    "Ys_dev = load_from_pickle(os.path.join(DATA_DIR, 'Y_dev.pkl'))\n",
    "Ys_test = load_from_pickle(os.path.join(DATA_DIR, 'Y_test.pkl'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "387bae31cbe00d99"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GAP_TIME          = 6  # In hours\n",
    "WINDOW_SIZE       = 24 # In hours\n",
    "SEED              = 69\n",
    "ID_COLS           = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "GPU               = '2'\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "251646796077e0bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "# GRU_D_dist = DictDist({\n",
    "#     'cell_size': ss.randint(50, 75),\n",
    "#     'hidden_size': ss.randint(65, 95),\n",
    "#     'learning_rate': ss.uniform(2e-3, 1e-1),\n",
    "#     'num_epochs': ss.randint(15, 150),\n",
    "#     'patience': ss.randint(3, 7),\n",
    "#     'batch_size': ss.randint(35, 65),\n",
    "#     'early_stop_frac': ss.uniform(0.05, 0.1),\n",
    "#     'seed': ss.randint(1, 10000),\n",
    "# })\n",
    "\n",
    "EARLY_STOP_FRAC = 0.05\n",
    "SEED = 69\n",
    "\n",
    "GRU_D_dist = DictDist({\n",
    "    'cell_size': ss.randint(50, 75),\n",
    "    'hidden_size': ss.randint(65, 95),\n",
    "    'learning_rate': ss.loguniform(1e-4, 1e-1),\n",
    "    'num_epochs': ss.randint(15, 150),\n",
    "    'patience': ss.randint(3, 20),\n",
    "    'batch_size': ss.randint(35, 65),\n",
    "    # 'early_stop_frac': 0.1,  # Or any specific value or narrow range\n",
    "    # 'seed': 69,  # Or any fixed seed value\n",
    "    # 'grad_clip_value': ss.uniform(0.5, 1.0)\n",
    "})\n",
    "\n",
    "np.random.seed(SEED)\n",
    "GRU_D_hyperparams_list = GRU_D_dist.rvs(N)\n",
    "\n",
    "# with open('../src/model/baselines_gru-d.pkl', mode='rb') as f: results = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18a1ee755e2a1584"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = {}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40560c181d1384de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name       = 'GRU-D'\n",
    "tasks             = ['mort_hosp']\n",
    "hyperparams_list = GRU_D_hyperparams_list\n",
    "RERUN            = False\n",
    "\n",
    "if model_name not in results: results[model_name] = {}\n",
    "\n",
    "# for t in tasks:         # Edit - DR\n",
    "#     if t not in results[model_name]: results[model_name][t] = {}\n",
    "t = 'mort_hosp'\n",
    "if t not in results[model_name]: results[model_name][t] = {}\n",
    "    # for n, X_train, X_dev, X_test in (('full_X', X_train, X_dev, X_test),):  # Edit - DR\n",
    "n, X_train, X_dev, X_test = ('full_X', X_train, X_dev, X_test)\n",
    "print(\"Running model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "X_mean = np.nanmean(\n",
    "    to_3D_tensor(\n",
    "        X_train.loc[:, pd.IndexSlice[:, 'mean']] *\n",
    "        np.where((X_train.loc[:, pd.IndexSlice[:, 'mask']] == 1).values, 1, np.NaN)\n",
    "    ),\n",
    "    axis=0, keepdims=True\n",
    ").transpose([0, 2, 1])\n",
    "base_params = {'X_mean': X_mean, 'output_last': True, 'input_size': X_mean.shape[2]}\n",
    "\n",
    "if n in results[model_name][t]:\n",
    "    if not RERUN:\n",
    "        print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "        print(results[model_name][t][n])\n",
    "        # continue\n",
    "    best_s, best_hyperparams = results[model_name][t][n][-1], results[model_name][t][n][1]\n",
    "    print(\"Loading best hyperparams\", best_hyperparams)\n",
    "else:\n",
    "    best_s, best_hyperparams = -np.Inf, None\n",
    "    for i, hyperparams in enumerate(hyperparams_list):\n",
    "        print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))\n",
    "\n",
    "        # early_stop_frac,batch_size,seed = [hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]    # Edit - DR\n",
    "        batch_size = hyperparams['batch_size']\n",
    "\n",
    "        np.random.seed(SEED)\n",
    "        all_train_subjects = list(\n",
    "            np.random.permutation(Ys_train.index.get_level_values('subject_id').values)\n",
    "        )\n",
    "        N_early_stop        = int(len(all_train_subjects) * EARLY_STOP_FRAC)\n",
    "        train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "        early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "        X_train_obs         = X_train[X_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "        Ys_train_obs        = Ys_train[Ys_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "        X_train_early_stop  = X_train[X_train.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "        Ys_train_early_stop = Ys_train[\n",
    "            Ys_train.index.get_level_values('subject_id').isin(early_stop_subjects)\n",
    "        ]\n",
    "        print(\"---------------------------------\")\n",
    "        print(\"Entering prepare_dataloader with inputs: \")\n",
    "        print(f\"X_train_obs.shape: {X_train_obs.shape}\")\n",
    "        print(f\"Ys_train_obs[t].shape: {Ys_train_obs[t].shape}\")\n",
    "        print(f\"batch_size: {batch_size}\")\n",
    "        train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "        early_stop_dataloader = prepare_dataloader(\n",
    "            X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size\n",
    "        )\n",
    "        dev_dataloader        = prepare_dataloader(X_dev, Ys_dev[t], batch_size=batch_size)\n",
    "        test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "        model_hyperparams = copy.copy(base_params)\n",
    "        model_hyperparams.update(\n",
    "            {k: v for k, v in hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "        )\n",
    "        # del model_hyperparams['cell_size']  # Edit - DR\n",
    "        model = GRUD(**model_hyperparams)\n",
    "        # model.apply(weights_init)  # Edit - DR\n",
    "        \n",
    "        # print('Debugging Parameters...')\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     print(name, torch.isnan(param).any(), torch.isinf(param).any())\n",
    "        #     if param.grad is not None:\n",
    "        #         print('Debugging Grad...')\n",
    "        #         print(name, torch.isnan(param.grad).any(), torch.isinf(param.grad).any())\n",
    "        \n",
    "        best_model, _ = Train_Model(\n",
    "            model, train_dataloader, early_stop_dataloader,\n",
    "            **{k: v for k, v in hyperparams.items() if k in (\n",
    "                'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "            )}\n",
    "        )\n",
    "        print('Debugging Parameters...')\n",
    "        for name, param in model.named_parameters():\n",
    "            print(name, torch.isnan(param).any(), torch.isinf(param).any())\n",
    "            if param.grad is not None:\n",
    "                print('Debugging Grad...')\n",
    "                print(name, torch.isnan(param.grad).any(), torch.isinf(param.grad).any())\n",
    "\n",
    "        probabilities_dev, labels_dev = predict_proba(best_model, dev_dataloader)\n",
    "        probabilities_dev = np.concatenate(probabilities_dev)[:, 1]\n",
    "        labels_dev        = np.concatenate(labels_dev)\n",
    "        s = roc_auc_score(labels_dev, probabilities_dev)\n",
    "        if s > best_s:\n",
    "            best_s, best_hyperparams = s, hyperparams\n",
    "            print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef43b24b97682a66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "hyperparams = hyperparams_list[i]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "430966ad694d93da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr(hyperparams)))\n",
    "\n",
    "# early_stop_frac,batch_size,seed = [hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]    # Edit - DR\n",
    "batch_size = hyperparams['batch_size']\n",
    "\n",
    "np.random.seed(SEED)\n",
    "all_train_subjects = list(\n",
    "    np.random.permutation(Ys_train.index.get_level_values('subject_id').values)\n",
    ")\n",
    "N_early_stop        = int(len(all_train_subjects) * EARLY_STOP_FRAC)\n",
    "train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "X_train_obs         = X_train[X_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "Ys_train_obs        = Ys_train[Ys_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "X_train_early_stop  = X_train[X_train.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "Ys_train_early_stop = Ys_train[\n",
    "    Ys_train.index.get_level_values('subject_id').isin(early_stop_subjects)\n",
    "]\n",
    "\n",
    "train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "early_stop_dataloader = prepare_dataloader(\n",
    "    X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size\n",
    ")\n",
    "dev_dataloader        = prepare_dataloader(X_dev, Ys_dev[t], batch_size=batch_size)\n",
    "test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "model_hyperparams = copy.copy(base_params)\n",
    "model_hyperparams.update(\n",
    "    {k: v for k, v in hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    ")\n",
    "# del model_hyperparams['cell_size']  # Edit - DR\n",
    "model = GRUD(**model_hyperparams)\n",
    "# model.apply(weights_init)  # Edit - DR\n",
    "\n",
    "# print('Debugging Parameters...')\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, torch.isnan(param).any(), torch.isinf(param).any())\n",
    "#     if param.grad is not None:\n",
    "#         print('Debugging Grad...')\n",
    "#         print(name, torch.isnan(param.grad).any(), torch.isinf(param.grad).any())\n",
    "\n",
    "best_model, _ = Train_Model(\n",
    "    model, train_dataloader, early_stop_dataloader,\n",
    "    **{k: v for k, v in hyperparams.items() if k in (\n",
    "        'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "    )}\n",
    ")\n",
    "print('Debugging Parameters...')\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, torch.isnan(param).any(), torch.isinf(param).any())\n",
    "    if param.grad is not None:\n",
    "        print('Debugging Grad...')\n",
    "        print(name, torch.isnan(param.grad).any(), torch.isinf(param.grad).any())\n",
    "\n",
    "probabilities_dev, labels_dev = predict_proba(best_model, dev_dataloader)\n",
    "probabilities_dev = np.concatenate(probabilities_dev)[:, 1]\n",
    "labels_dev        = np.concatenate(labels_dev)\n",
    "s = roc_auc_score(labels_dev, probabilities_dev)\n",
    "if s > best_s:\n",
    "    best_s, best_hyperparams = s, hyperparams\n",
    "    print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e90edaa31cff6516"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
